{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trg = pd.read_csv(\"H:\\\\COMPSCI 361\\\\trg.csv\", index_col = 0)\n",
    "\n",
    "# split each abstract value into a list:\n",
    "for i in range(len(trg)):\n",
    "    trg.iloc[i,1] = trg.iloc[i,1].split()\n",
    "    \n",
    "training = trg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dataset):\n",
    "    \n",
    "    # split the trg dataframe into 4 dataframes for each class\n",
    "    df_groupby_classes = [v for k, v in trg.groupby('class')] # list of dataframes\n",
    "    \n",
    "    # remove useless words:\n",
    "    remove_more = ['with', 'that', 'from', 'were', 'which', 'that', 'have', 'these', 'been', 'other', 'the', 'this', 'found', 'more', 'three', 'also', 'only', 'open', 'there']\n",
    "    for X_df in df_groupby_classes: # X_df is a single a dataframe\n",
    "        for i in range(len(X_df)): # range(len(X_df)) is an int\n",
    "            for word in X_df.iloc[i,1]: \n",
    "                if len(word) < 4 or word in remove_more:\n",
    "                    X_df.iloc[i,1].pop(X_df.iloc[i,1].index(word))\n",
    "\n",
    "    # get frequency of words in each classes\n",
    "    word_count_per_classes = [dict(),dict(),dict(),dict()]\n",
    "    for n in range(4):\n",
    "        for i in range(len(df_groupby_classes[n])): # this is the length of the nth class dataframe\n",
    "            for each in df_groupby_classes[n].iloc[i,1]: # each word in the ith row of the nth dataframe\n",
    "                if each not in word_count_per_classes[n].keys(): #  key of the nth dictionary\n",
    "                    word_count_per_classes[n][each] = 1\n",
    "                else:\n",
    "                    word_count_per_classes[n][each] +=1\n",
    "                    \n",
    "    # get the n most commonly occurring words for each of the classes\n",
    "    words_class_sorted = [[],[],[],[]]\n",
    "    for n in range(4):\n",
    "        words_class_sorted[n] = sorted(word_count_per_classes[n].items(), key=lambda x: x[1], reverse= True)\n",
    "        \n",
    "    most_common_words_by_class = [[],[],[],[]]\n",
    "    for n in range(4):\n",
    "        end = int(len(words_class_sorted[n])*0.33)\n",
    "        for i in range(end):\n",
    "            most_common_words_by_class[n].append(words_class_sorted[n][0:end][i][0])\n",
    "            \n",
    "    # use sets to remove common words that are in all four dictionaries and therefore get a list of features that are composed of only unique words from each class type:\n",
    "    a,b,e,v = set(most_common_words_by_class[0]), set(most_common_words_by_class[1]), set(most_common_words_by_class[2]), set(most_common_words_by_class[3])\n",
    "    features = list((a|b|e|v) - ((a&b)|(a&e)|(a&v)|(b&e)|(b&v)|(e&v)) - ((a&b&e)|(a&b&v)|(b&e&v)) - (a&b&e&v))\n",
    "    # = list(((a-(b|e|v))|(b-(a|e|v))|(e-(a|b|v))|(v-(a|b|e)))|((a&b&e)|(a&b&v)|(b&e&v)|(a&e&v)))\n",
    "    if \"class\" in features:\n",
    "        features.remove(\"class\")\n",
    "        \n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = get_features(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate will eventually return the result\n",
    "def cross_validate(training, features, folds):\n",
    "    start = 0\n",
    "    end = int(len(training)/folds)\n",
    "    for i in range(folds):\n",
    "        Y_class = training[start:end].drop('abstract',1)\n",
    "        Y_train = training[start:end].drop('class',1)\n",
    "        X_train = training.drop(Y_train.index, inplace=False)\n",
    "        \n",
    "        df = X_train.copy()\n",
    "        for x in features:\n",
    "            df[x] = 0\n",
    "        df = df.drop('abstract',1)\n",
    "        \n",
    "        for i in range(len(X_train)):\n",
    "            for word in X_train.iloc[i,1]:\n",
    "                if word in features:\n",
    "                    colnumber = features.index(word) + 1\n",
    "                    df.iloc[i,colnumber] += 1\n",
    "                    \n",
    "        class_priors = {'A':0, 'B':0, 'E':0, 'V':0}\n",
    "        total_class_frequencies = df['class'].value_counts(sort = False)\n",
    "        for key in class_priors:\n",
    "            class_priors[key] = (total_class_frequencies[key]+1)/(sum(total_class_frequencies)+4)\n",
    "        \n",
    "        sorted_df = [v for k, v in df.groupby('class')]\n",
    "        sorted_A = sorted_df[0]\n",
    "        sorted_B = sorted_df[1]\n",
    "        sorted_E = sorted_df[2]\n",
    "        sorted_V = sorted_df[3]\n",
    "\n",
    "        # P(xi|A):\n",
    "        xgivenA = {}\n",
    "        total_word_count_xgivenA = len(features)\n",
    "        for x in features:\n",
    "            xgivenA[x] = sum(sorted_A[x])+1\n",
    "            total_word_count_xgivenA += xgivenA[x]\n",
    "\n",
    "        for x in xgivenA:\n",
    "            xgivenA[x] = xgivenA[x] / (total_word_count_xgivenA+1)\n",
    "\n",
    "        # P(xi|B):\n",
    "        xgivenB = {}\n",
    "        total_word_count_xgivenB = len(features)\n",
    "        for x in features:\n",
    "            xgivenB[x] = sum(sorted_B[x])+1\n",
    "            total_word_count_xgivenB += xgivenB[x]\n",
    "\n",
    "        for x in xgivenA:\n",
    "            xgivenB[x] = xgivenB[x] / (total_word_count_xgivenB+1)\n",
    "\n",
    "        # P(xi|E):\n",
    "        xgivenE = {}\n",
    "        total_word_count_xgivenE = len(features)\n",
    "        for x in features:\n",
    "            xgivenE[x] = sum(sorted_E[x])+1\n",
    "            total_word_count_xgivenE += xgivenE[x]\n",
    "\n",
    "        for x in xgivenE:\n",
    "            xgivenE[x] = xgivenE[x] / (total_word_count_xgivenE+1)\n",
    "\n",
    "        # P(xi|V):\n",
    "        xgivenV = {}\n",
    "        total_word_count_xgivenV = len(features)\n",
    "        for x in features:\n",
    "            xgivenV[x] = sum(sorted_V[x])+1\n",
    "            total_word_count_xgivenV += xgivenV[x]\n",
    "\n",
    "        for x in xgivenV:\n",
    "            xgivenV[x] = xgivenV[x] / (total_word_count_xgivenV+1)\n",
    "            \n",
    "        import math\n",
    "\n",
    "        # caluclate the probability that example is some class Ck given that we have the set of counts:\n",
    "\n",
    "        def get_priori(word):\n",
    "            priori_A = xgivenA[word]\n",
    "            priori_B = xgivenB[word]\n",
    "            priori_E = xgivenE[word]\n",
    "            priori_V = xgivenV[word]\n",
    "            return priori_A, priori_B, priori_E, priori_V\n",
    "\n",
    "        def get_multiplicative_total_probability_of_x_given_class(x):\n",
    "            # x = {x1, ... , xn}\n",
    "            total_A = class_priors['A']\n",
    "            total_B = class_priors['B']\n",
    "            total_E = class_priors['E']\n",
    "            total_V = class_priors['V']\n",
    "            for xi in x:\n",
    "                if xi in features:\n",
    "                    total_A *= xgivenA[xi]\n",
    "                    total_B *= xgivenB[xi]\n",
    "                    total_E *= xgivenE[xi]\n",
    "                    total_V *= xgivenV[xi]\n",
    "            return math.log(total_A),math.log(total_B), math.log(total_E), math.log(total_V)\n",
    "\n",
    "        predicted = []\n",
    "        for i in range(len(Y_train)):\n",
    "            classes_list = ['A','B','E','V']\n",
    "            for x in Y_train.iloc[i,0:]:\n",
    "                pa,pb,pe,pv = get_multiplicative_total_probability_of_x_given_class(x)\n",
    "                predicted.append(classes_list[([pa,pb,pe,pv].index(max(pa,pb,pe,pv)))])\n",
    "                \n",
    "        # calculate the error of my predictions against the training set\n",
    "        error = 0 \n",
    "        for each in list(zip(predicted,Y_class)):\n",
    "            if each[0] != each[1]:\n",
    "                error +=1\n",
    "\n",
    "        print(error)\n",
    "        print(100 - error/len(Y_class)*100)\n",
    "        start = end\n",
    "        end += int(len(training)/folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "99.95\n"
     ]
    }
   ],
   "source": [
    "cross_validate(training,features,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
